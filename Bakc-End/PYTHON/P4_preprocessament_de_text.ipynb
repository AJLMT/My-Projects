{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4: preprocessament i extracció de característiques\n",
    "En aquesta pràctica treballarem en el preprocessament de text i en l'extracció de característiques *sparse* i denses dels documents.\n",
    "\n",
    "**Alumne 1**: <label style=\"color:green\"> Alejandro Madrid Galarza </label>\n",
    "\n",
    "**Alumne 2**: <label style=\"color:green\"> Antonio José López Martínez </label>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajlmt\\AppData\\Local\\Temp\\ipykernel_14416\\608376737.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.add_pipe(\"merge_entities\") # Ajuntem els tokens de cada entitat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: preprocessament de text\n",
    "En aquesta part construirem una funció generadora per carregar els documents de text des d'una carpeta de disc, i realitzarem un processament inicial a cada document.  \n",
    "Farem servir un conjunt de notícies esportives del Marca, obtingut de https://www.kaggle.com/datasets/mdamsterdam/marca-spanish-sports-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def carrega_textos(path):\n",
    "    \"\"\"Funció generadora que carrega els fitxers de tipus TXT d'una carpeta.\n",
    "    Retorna (yield) el text següent en cada execució.\"\"\"\n",
    "\n",
    "    for file in [f for f in os.listdir(path) if f.endswith('.txt')]:\n",
    "        with open(os.path.join(path, file), encoding='utf-8') as f:\n",
    "            \n",
    "            yield f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova aquesta funció carregant el primer fitxer de la carpeta de materials de la pràctica (`\"P4_materials/noticies\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campazzo y Tavares llevan al Real Madrid a su séptima final de Copa seguida El Unicaja ya está en \"su\" final tras aplastar al MoraBanc Ibai Llanos la \"lía\" allá por donde va. Esta vez ha sido con un lanzamiento desde medio campo. El youtuber, que andaba conversando con su amigo Facundo Campazzo a su llegada al Martín Carpena, ha vuelto a armarla.  QUIÉN ES CURRY NO ME SUENA DE NADA pic.twitter.com/8gS6V8JXqp Solo tienen que ver la reacción tras su canasta anotada desde el centro de la cancha. \"¿Quién es Curry?¡No le conozco!\", ha gritado. Un ídolo de masas.  Suscríbete a la Newsletter de Basket de MARCA y recibe en tu correo electrónico, de lunes a domingo y a primera hora de la mañana, las noticias exclusivas, entrevistas, reportajes, gráficos y vídeos que marcarán el día en la NBA, Liga Endesa, Euroliga y el resto del mundo de la canasta.\n"
     ]
    }
   ],
   "source": [
    "ruta = \"P4_materials/noticies\"\n",
    "gen = carrega_textos(ruta)\n",
    "text = next(gen)\n",
    "print(text[:900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalització de text.\n",
    "Crea una funció per normalitzar el text de cada notícia. Els passos a realitzar per la funció seran:  \n",
    "- Eliminar *stop-words* i signes de puntuació.\n",
    "- Extreure el lema de cada paraula.\n",
    "- Anonimitzar el text (substituir les entitats de tipus `PER` pel text \"PERSONA\").  \n",
    "\n",
    "Utilitza la llibreria `spacy` amb el model `ES` per fer la normalització. La funció accepta com a entrada un *string* i torna a la sortida també un *string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "def normalitza(text):\n",
    "    \"\"\"Funció que normalitza un string de text.\n",
    "    Entrada: string a normalitzar.\n",
    "    Retorna: string del text normalitzat.\n",
    "\"\"\"\n",
    "    # Processament del text amb SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Eliminació de stop-words i signes de puntuació, lematització i anonimització\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        if token.ent_type_ == 'PER':\n",
    "            tokens.append('PERSONA')\n",
    "        else:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    # Retornem el text normalitzat\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proveu aquesta funció sobre la primera notícia de la carpeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalitzat:\n",
      "PERSONA PERSONA llevar Real Madrid séptimo Copa seguido el Unicaja aplastar MoraBanc Ibai Llanos lía allá lanzamiento campo el youtuber andar conversar amigo PERSONA llegada PERSONA volver armar él   CURRY SUENA pic.twitter.com/8gs6v8jxqp reacción canasta anotado centro cancha ¿quién ser Curry?¡No él conocer! gritar uno ídolo de masa   Suscríbete a el Newsletter de Basket MARCA recibir correo electrónico lunes domingo hora mañana noticia exclusivo entrevista reportaje gráfico vídeo marcar NBA Liga Endesa Euroliga resto mundo canasta\n"
     ]
    }
   ],
   "source": [
    "ruta = \"P4_materials/noticies\"\n",
    "gen = carrega_textos(ruta)\n",
    "text = next(gen)\n",
    "\n",
    "normalitzat = normalitza(text)\n",
    "print(\"Text normalitzat:\")\n",
    "print(normalitzat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: extracció de característiques globals.\n",
    "Extreurem un conjunt de característiques globals de cada document:  \n",
    "- Longitud del text.\n",
    "- Nombre de paraules i frases.\n",
    "- Nombre d'entitats a cada text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracció de característiques.\n",
    "Crea una funció que a partir d'un text d'entrada (objecte `string`) genere un diccionari de característiques amb els valors següents:  \n",
    "- `caracters`: longitud del text en caràcters.\n",
    "- `paraules`: nombre de paraules del text excloent tokens de puntuació.\n",
    "- `frases`: nombre de frases del text.\n",
    "- `ENT_PER`: nº d'entitats de tipus `PER` al text.\n",
    "- `ENT_LOC`: nº d'entitats de tipus `LOC` al text\n",
    "- `ENT_ORG`: nº d'entitats de tipus `ORG` al text.\n",
    "- `ENT_MISC`: nº d'entitats de tipus `MISC` al text.  \n",
    "\n",
    "Ajuda: utilitza la classe `Counter` de la llibreria `collections` per comptar el nombre d'entitats de cada tipus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def caracteristiques(text):\n",
    "    \"\"\"Calcula una sèrie de característiques d'un text i les retorna com a valors d'un objecte diccionari.\"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    entitats = Counter([ent.label_ for ent in doc.ents])\n",
    "    paraules = [token.text.lower() for token in doc if not token.is_punct]\n",
    "    num_paraules = len(paraules)\n",
    "    num_frases = len(list(doc.sents))\n",
    "    \n",
    "    carac_dict = {\n",
    "        'caracters': len(text),\n",
    "        'paraules': num_paraules,\n",
    "        'frases': num_frases,\n",
    "        'ENT_PER': entitats['PER'],\n",
    "        'ENT_LOC': entitats['LOC'],\n",
    "        'ENT_ORG': entitats['ORG'],\n",
    "        'ENT_MISC': entitats['MISC']\n",
    "    }\n",
    "    \n",
    "    return carac_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova-ho sobre el primer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text caracteritzat:\n",
      "{'caracters': 852, 'paraules': 132, 'frases': 8, 'ENT_PER': 4, 'ENT_LOC': 2, 'ENT_ORG': 4, 'ENT_MISC': 8}\n"
     ]
    }
   ],
   "source": [
    "ruta = \"P4_materials/noticies\"\n",
    "gen = carrega_textos(ruta)\n",
    "text = next(gen)\n",
    "\n",
    "carac = caracteristiques(text)\n",
    "print(\"Text caracteritzat:\")\n",
    "print(carac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un objecte DataFrame de `pandas` amb aquestes característiques per a tots els textos dins la carpeta de materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   caracters  paraules  frases  ENT_PER  ENT_LOC  ENT_ORG  ENT_MISC\n",
      "0        852       132       8        4        2        4         8\n",
      "1        716       112       4        3        3        5         6\n",
      "2        725       119       4        2        1        4         6\n",
      "3       4378       669      45       23       15        2        32\n",
      "4       3214       502      33       17       10        4        20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "textos = list(carrega_textos(\"P4_materials/noticies\"))\n",
    "caracteristiques_texts = []\n",
    "\n",
    "for text in textos:\n",
    "    carac = caracteristiques(text)\n",
    "    caracteristiques_texts.append(carac)\n",
    "\n",
    "df_caracteristiques = pd.DataFrame(caracteristiques_texts)\n",
    "print(df_caracteristiques.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analitza les estadístiques d'aquest DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          caracters     paraules      frases     ENT_PER     ENT_LOC  \\\n",
      "count    561.000000   561.000000  561.000000  561.000000  561.000000   \n",
      "mean    2429.572193   379.433155   19.146168   14.775401   10.718360   \n",
      "std     1652.645616   257.481926   20.291713   15.695979   10.192741   \n",
      "min      124.000000    22.000000    1.000000    0.000000    0.000000   \n",
      "25%     1429.000000   222.000000    8.000000    6.000000    4.000000   \n",
      "50%     2072.000000   327.000000   14.000000   11.000000    8.000000   \n",
      "75%     3017.000000   468.000000   21.000000   18.000000   14.000000   \n",
      "max    16002.000000  2297.000000  196.000000  180.000000   66.000000   \n",
      "\n",
      "          ENT_ORG    ENT_MISC  \n",
      "count  561.000000  561.000000  \n",
      "mean     4.427807   14.641711  \n",
      "std      4.537803   14.956901  \n",
      "min      0.000000    0.000000  \n",
      "25%      2.000000    7.000000  \n",
      "50%      3.000000   11.000000  \n",
      "75%      6.000000   16.000000  \n",
      "max     41.000000  137.000000  \n"
     ]
    }
   ],
   "source": [
    "estadistiques = df_caracteristiques.describe()\n",
    "print(estadistiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: extracció de característiques *sparse*.\n",
    "Obtindrem una matriu Bag-of-Words i TF-IDF sobre el corpus anterior (conjunt de notícies del Marca), després de fer una neteja prèvia a cada document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\") # Carreguem per eliminar el merge_entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neteja del text\n",
    "Defineix una funció de neteja sobre un string que faça les accions següents:  \n",
    "- Eliminar stop-words i signes de puntuació.  \n",
    "- Eliminar aquelles paraules el `POS` de les quals no sigui ni nom, adjectiu o verb.\n",
    "- Extreure el lema de cada paraula.\n",
    "- Convertir en minúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neteja(text):\n",
    "    \"\"\"Realitza una neteja d'un text i torna el text netejat com a string\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    paraules_excloure = set()\n",
    "    for token in doc:\n",
    "        if not token.pos_ in ['NOUN', 'ADJ', 'VERB']:\n",
    "            paraules_excloure.add(token.text.lower())\n",
    "    \n",
    "    tokens_net = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or token.text.lower() in paraules_excloure:\n",
    "            continue\n",
    "        tokens_net.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join(tokens_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proveu la funció sobre el primer article de notícies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text netejat:\n",
      "llevar séptimo seguido aplastar lía lanzamiento campo youtuber andar conversar amigo llegada volver armar él pic.twitter.com/8gs6v8jxqp reacción canasta anotado centro cancha conocer gritar ídolo masa recibir correo electrónico lunes domingo hora mañana noticia exclusivo entrevista reportaje gráfico vídeo marcar resto mundo canasta\n"
     ]
    }
   ],
   "source": [
    "ruta = \"P4_materials/noticies\"\n",
    "gen = carrega_textos(ruta)\n",
    "text = next(gen)\n",
    "\n",
    "net = neteja(text)\n",
    "print(\"Text netejat:\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriu Bag-of-Words.\n",
    "Crea amb la llibreria `sklearn` la matriu BoW de tots els fitxers de notícies del directori de pràctiques a partir dels textos netejats. Utilitza la funció `map` per crear un iterador amb els fitxers netejats mitjançant la funció anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m ruta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP4_materials/noticies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m textos_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m file: neteja(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ruta, file), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()), os\u001b[38;5;241m.\u001b[39mlistdir(ruta))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ruta = \"P4_materials/noticies\"\n",
    "textos_net = map(lambda file: neteja(open(os.path.join(ruta, file), encoding='utf-8').read()), os.listdir(ruta))\n",
    "\n",
    "vectoritzador = CountVectorizer()\n",
    "matriu_bow = vectoritzador.fit_transform(textos_net)\n",
    "print(\"Matriu Bag-of-Words\")\n",
    "print(matriu_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra les dimensions de la matriu generada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriu_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anàlisi de les paraules més freqüents.\n",
    "Calcula la freqüència d'ús de cada paraula (no la freqüència de documents) i guarda'l un DataFrame amb les columnes \"paraules\" i \"freqüència\", ordena-les de major a menor i mostra les 20 més freqüents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import arg\n",
    "vocabulario = vectoritzador.get_feature_names_out()\n",
    "\n",
    "# Sumar las ocurrencias de cada palabra en toda la bolsa de palabras\n",
    "frecuencia_palabras = matriu_bow.sum(axis=0)\n",
    "\n",
    "# Obtener las frecuencias y palabras correspondientes\n",
    "frecuencia_palabras = frecuencia_palabras.tolist()[0]\n",
    "palabras = [vocabulario[i] for i in range(len(vocabulario))]\n",
    "\n",
    "# Crear un DataFrame con las columnas \"palabras\" y \"frecuencia\"\n",
    "df_frecuencia = pd.DataFrame({\"palabras\": palabras, \"frecuencia\": frecuencia_palabras})\n",
    "\n",
    "# Ordenar el DataFrame por frecuencia en orden descendente\n",
    "df_frecuencia = df_frecuencia.sort_values(by=\"frecuencia\", ascending=False)\n",
    "\n",
    "# Mostrar las 20 palabras más frecuentes\n",
    "print(df_frecuencia.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: extracció de característiques denses.\n",
    "Utilitzarem els *word embeddings* de les paraules de cada document per generar un vector dens de document. Amb aquests vectors implementarem un classificador *zero-shot* sobre els documents.  \n",
    "Utilitzarem ací un conjunt de dades que representa un carretó de compra, que intentarem associar a diferents categories representades per un text descriptiu.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càrrega de *word embeddings*.\n",
    "Farem servir els word embeddings en castellà de https://github.com/dccuchile/spanish-word-embeddings en format Word2Vec de Gensim.  \n",
    "Carregueu els vectors fent servir la classe `KeyedVectors` de `Gensim` a l'objecte `wordvectors`.\n",
    "\n",
    "Nota: Per a instal·lar el mòdul, utilitzeu `pip install -U gensim` una vegada activat el vostre entorn de `conda` i reinicieu el kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ruta del fitxer de vectors de paraules\n",
    "ruta_vectors = \"path_to_vectors/SBW-vectors-300-min5.bin\"  # Substitueix \"path_to_vectors\" per la ruta correcta del fitxer\n",
    "\n",
    "# Carreguem els vectors de paraules\n",
    "wordvectors = KeyedVectors.load_word2vec_format(ruta_vectors, binary=True)\n",
    "\n",
    "# Exemple d'ús:\n",
    "vector_rei = wordvectors[\"rei\"]  # Vector de la paraula \"rei\"\n",
    "print(\"Dimensionalitat del vector:\", len(vector_rei))  # Imprimeix la dimensionalitat del vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a exemple, busquem les paraules més similars semànticament a \"automóvil\" segons la seva similitud cosinus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors.most_similar(['automóvil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim una funció per calcular el *sentence embedding* d'un text com la mitjana dels *word embeddings* de les seves paraules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm # Per normalitzar dades.\n",
    "\n",
    "def to_vector(text):\n",
    "    \"\"\"Calcula el vector dens del text com a mitjana dels word embeddings de les paraules\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    vec = np.zeros(300) # L'embedding té un tamany de '300'.\n",
    "    for word in tokens:\n",
    "        # Si la paraula està l'acumulem.\n",
    "        if word in wordvectors:\n",
    "            vec += wordvectors[word]\n",
    "    return vec / norm(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova aquesta funció per calcular la similitud cosinus entre les frases '*el partido de fútbol*', '*deportes de equipo*' i '*noticias internacionales*'.  \n",
    "Defineix per a això una funció `similarity` sobre dos *strings* d'entrada usant l'operador producte matricial de NumPy (`@`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(text_1, text_2):\n",
    "    \"\"\"Calcula la similitud cosinus entre dos textos\n",
    "    com el producte matricial dels seus vectors densos\n",
    "    calculats amb la funció to_vector()\"\"\"\n",
    "    \n",
    "    vec_1 = to_vector(text_1)\n",
    "    vec_2 = to_vector(text_2)\n",
    "    cos_sim = vec_1 @ vec_2\n",
    "    \n",
    "    return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la similitud entre els 3 textos.\n",
    "texto_1 = \"el partido de fútbol\"\n",
    "texto_2 = \"deportes de equipo\"\n",
    "texto_3 = \"noticias internacionales\"\n",
    "\n",
    "salida = similarity(texto_1, texto_2)\n",
    "print(salida)\n",
    "salida = similarity(texto_1, texto_3)\n",
    "print(salida)\n",
    "salida = similarity(texto_2, texto_3)\n",
    "print(salida)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifica la funció `to_vector` per eliminar paraules que siguen dígits (mètode `str.isdigit()`) o la longitud de les quals siga menor a 3 caràcters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def to_vector(text):\n",
    "    \"\"\"Calcula el vector dens del text com a mitjana dels word embeddings de les paraules,\n",
    "    eliminant paraules que siguin dígits o la longitud de les quals siga menor a 3 caràcters.\"\"\"\n",
    "    \n",
    "    tokens = text.lower().split()\n",
    "    vec = np.zeros(300) # L'embedding té una dimensió de '300'.\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word.isdigit() or len(word) < 3:\n",
    "            continue\n",
    "        \n",
    "        if word in wordvectors:\n",
    "            vec += wordvectors[word]\n",
    "    \n",
    "    if norm(vec) != 0:\n",
    "        vec /= norm(vec)\n",
    "    \n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprova la similitud als 3 textos anteriors amb la nova funció de vectorització."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m texto_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeportes de equipo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m texto_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoticias internacionales\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m salida \u001b[38;5;241m=\u001b[39m \u001b[43mto_vector\u001b[49m(texto_1)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(salida)\n\u001b[0;32m      8\u001b[0m salida \u001b[38;5;241m=\u001b[39m to_vector(texto_3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_vector' is not defined"
     ]
    }
   ],
   "source": [
    "# Calcula la similitud entre els 3 textos.\n",
    "texto_1 = \"el partido de fútbol\"\n",
    "texto_2 = \"deportes de equipo\"\n",
    "texto_3 = \"noticias internacionales\"\n",
    "\n",
    "salida = to_vector(texto_1)\n",
    "print(salida)\n",
    "salida = to_vector(texto_3)\n",
    "print(salida)\n",
    "salida = to_vector(texto_2)\n",
    "print(salida)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ús de vectors densos en un problema de classificació *zero-shot*.\n",
    "Carregarem un llistat de compres etiquetat al costat de la seua categoria, definida com una llista de termes. Farem servir la similitud cosinus entre cada compra i les categories per classificar-la automàticament.  \n",
    "Les dades de les compres són al fitxer `dataset_compras.csv` i les seves classes al fitxer `classes.txt`. Carrega tots dos fitxers al DataFrame `compres` i la llista `classes` respectivament. L'ordre de les classes a l'arxiu es correspon a l'enter assignat a cada classe a `compres`.\n",
    "\n",
    "Cada línia del fitxer *classes.txt* correspon a una etiqueta diferent. Hi ha un total de 7 línies, pel que cada línia correspondrà a un enter del CSV de les compres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeres files del DataFrame compres:\n",
      "                               compra  clase\n",
      "0                      adhesivo mayon      4\n",
      "1                     cerveza el lata      1\n",
      "2  pasaje en avión santiago balmaceda      5\n",
      "3  pasaje ida y vuelta cqob la serena      5\n",
      "4                 reparación de rueda      5\n",
      "\n",
      "Classes:\n",
      "['alimentos comida bebida carne pollo jugo', 'alcohol cigarrillo tabaco', 'ropa de vestir calzado zapatos vestidos', 'muebles hogar aseo herramienta', 'salud medicamento hospital', 'transporte bus aviÃ³n automÃ³vil', 'comunicaciones telÃ©fono celular']\n"
     ]
    }
   ],
   "source": [
    "# Carreguem compres i classes.\n",
    "compres = pd.read_csv(\"compres_subset.csv\")\n",
    "\n",
    "# Carreguem la llista de classes\n",
    "with open(\"classes.txt\", \"r\") as file:\n",
    "    classes = file.readlines()\n",
    "\n",
    "# Eliminem els caràcters de nova línia (\\n) de cada element de la llista de classes\n",
    "classes = [classe.strip() for classe in classes]\n",
    "\n",
    "# Mostrem les primeres files del DataFrame compres i la llista classes\n",
    "print(\"Primeres files del DataFrame compres:\")\n",
    "print(compres.head())\n",
    "print(\"\\nClasses:\")\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agafa una compra a l'atzar i calcula la classe predita mitjançant la similitud cosinus entre el text de la compra i el text de cada classe.  \n",
    "Nota: crea un array a NumPy amb les similituds cosinus de la compra a cada classe i calcula la posició del màxim amb `np.argmax()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càlcul de *doc embeddings*.\n",
    "Genera els vectors de tots els ítems del DataFrame de compres i carrega'ls en un array 2D de NumPy usant `np.vstack()` anomenat `X`.  \n",
    "Fes el mateix amb les classes en un array anomenat `Y`.  \n",
    "Mostra la mida dels dos arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculem la similitud cosinus de cada compra amb totes les classes\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculem la similitud cosinus de cada compra amb totes les classes\n",
    "similituds_cosinus = cosine_similarity(X, Y.T)\n",
    "\n",
    "# Obtenim la classe predita per a cada compra com l'índex del màxim per files\n",
    "prediccions_classe = np.argmax(similituds_cosinus, axis=1)\n",
    "\n",
    "# Calculem les mètriques del classificador\n",
    "print(classification_report(y_true, prediccions_classe, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula el producte matricial entre `X` i la transposada de `Y` per obtenir la similitud cosinus de cada compra a totes les classes.  \n",
    "Obtin la predicció de classe assignada a cada compra com a índex del màxim per files (funció `np.argmax()`) i calcula les mètriques del classificador usant `classification_report` de la llibreria `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calculem la similitud cosinus de cada compra amb totes les classes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m similituds_cosinus \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m Y\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculem la similitud cosinus de cada compra amb totes les classes\n",
    "similituds_cosinus = X @ Y.T\n",
    "\n",
    "# Obtenim la classe predita per a cada compra com l'índex del màxim per files\n",
    "prediccions_classe = np.argmax(similituds_cosinus, axis=1)\n",
    "\n",
    "# Calculem les mètriques del classificador\n",
    "print(classification_report(y_true, prediccions_classe, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeteix la classificació usant el vectoritzador millorat `to_vector_plus` i compara els resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector_plus(text):\n",
    "    \"\"\"Calcula el vector dens del text com a mitjana dels word embeddings de les paraules,\n",
    "    eliminant paraules que siguin dígits, la longitud de les quals siga menor a 3 caràcters\n",
    "    i normalitzant el vector final.\"\"\"\n",
    "    \n",
    "    tokens = text.lower().split()\n",
    "    vec = np.zeros(300) # L'embedding té una dimensió de '300'.\n",
    "    \n",
    "    for word in tokens:\n",
    "        # Si la paraula és un dígit o la seva longitud és menor a 3, la saltem.\n",
    "        if word.isdigit() or len(word) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Si la paraula està en els word embeddings, l'acumulem.\n",
    "        if word in wordvectors:\n",
    "            vec += wordvectors[word]\n",
    "    \n",
    "    # Normalitzem el vector\n",
    "    if norm(vec) != 0:\n",
    "        vec /= norm(vec)\n",
    "    \n",
    "    return vec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
