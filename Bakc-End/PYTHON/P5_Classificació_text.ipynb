{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biblical-system",
   "metadata": {},
   "source": [
    "# Pràctica 5 PLN: classificació de text (multiclasse).\n",
    "En aquesta pràctica entrenarem un classificador de text per a diversos problemes multiclasse.\\\n",
    "Provarem amb una extracció de característiques vectorials *sparse* i amb una mitjana de *word vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-malpractice",
   "metadata": {},
   "source": [
    "### Noms:\n",
    "Alejandro Madrid Galarza \\\n",
    "Antonio José López Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c7b12",
   "metadata": {},
   "source": [
    "## Part 0: Actualitzar paqueteria `conda`i instal·lar model llenguatge.\n",
    "Executar les següents sentències a la terminal per a instal·lar el model de llenguatge que usarem en aquesta pràctica.\n",
    "\n",
    "> `python -m spacy download en_core_web_md`\n",
    "> `python -m spacy download es_core_news_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-wages",
   "metadata": {},
   "source": [
    "## Part 1: Conjunt de textos de \"mundocine\".\n",
    "En aquesta part el conjunt de dades que utilitzarem consisteix en una sèrie de crítiques de pel·lícules de cinema, emmagatzemades en format XML (una crítica per arxiu). Hem preparat una funció de tipus `generator` que processa el directori on estan els arxius de les crítiques i torna per cada arxiu XML una tupla amb 4 valors:\n",
    " - Nom de la pel·lícula (*string*)\n",
    " - Resum breu de la crítica (*string*)\n",
    " - Text de la crítica (*string*)\n",
    " - Valoració de la pel·lícula (*int* de 1 a 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "expensive-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "def parse_folder(path):\n",
    "    \"\"\"Generador que llegeix el contingut dels fitxers XML d'una carpeta.\n",
    "    Retorna el <body> de la <review> a cada fitxer XML.\n",
    "    Fitxers XML codificats com a 'latin-1'\"\"\"\n",
    "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
    "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
    "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
    "            doc=parseString(re.sub(r'(<>)|&|(<-)', '', f.read()))\n",
    "\n",
    "            titulo = doc.documentElement.attributes[\"title\"].value\n",
    "\n",
    "            btxt = \"\"\n",
    "            review_bod = doc.getElementsByTagName(\"body\")\n",
    "            if len(review_bod) > 0:\n",
    "                for node in review_bod[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        btxt += node.data + \" \"\n",
    "\n",
    "            rtxt = \"\"\n",
    "            review_summ = doc.getElementsByTagName(\"summary\")\n",
    "            if len(review_summ) > 0:\n",
    "                for node in review_summ[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        rtxt += node.data + \" \"\n",
    "                        \n",
    "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
    "            \n",
    "            yield titulo, rtxt, btxt, rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-villa",
   "metadata": {},
   "source": [
    "### Extracció de característiques.\n",
    "Tenim dos directoris, un directori de train (`\"critiques/train\"`) i un altre directori de test (`\"critiques/test\"`).\\\n",
    "Abans de processar el text calcularem una sèrie de paràmetres de cada crítica. \\\n",
    "Per això processem cada crítica per i guardarem els resultats en un objecte `DataFrame` de Pandes.\\\n",
    "Com a característica de cada crítica extreurem:\n",
    "- Títol de la pel·lícula.\n",
    "- Longitud (en caràcters) del resum.\n",
    "- Longitud (en caràcters) del text de la crítica.\n",
    "- Puntuació de la crítica.\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent per generar el `DataFrame` sobre el conjunt de train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "desirable-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creem una llista en blanc.\n",
    "dades = []\n",
    "\n",
    "# Recorrem les crítiques i calculem les seves mètriques.\n",
    "for c in parse_folder('critiques/critiques/train'):\n",
    "    dades.append({\n",
    "        'títol': c[0],\n",
    "        'LongResum': len(c[1]),\n",
    "        'LongCritica': len(c[2]),\n",
    "        'puntuació': c[3]\n",
    "    })\n",
    "\n",
    "resum = pd.DataFrame(dades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "silent-serbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LongResum</th>\n",
       "      <th>LongCritica</th>\n",
       "      <th>puntuació</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2714.000000</td>\n",
       "      <td>2714.000000</td>\n",
       "      <td>2714.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>173.058585</td>\n",
       "      <td>2805.424834</td>\n",
       "      <td>3.060796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.891358</td>\n",
       "      <td>1675.839317</td>\n",
       "      <td>1.141479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>129.000000</td>\n",
       "      <td>1751.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>173.000000</td>\n",
       "      <td>2358.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>217.000000</td>\n",
       "      <td>3427.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>425.000000</td>\n",
       "      <td>26668.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LongResum   LongCritica    puntuació\n",
       "count  2714.000000   2714.000000  2714.000000\n",
       "mean    173.058585   2805.424834     3.060796\n",
       "std      61.891358   1675.839317     1.141479\n",
       "min      12.000000      0.000000     1.000000\n",
       "25%     129.000000   1751.000000     2.000000\n",
       "50%     173.000000   2358.500000     3.000000\n",
       "75%     217.000000   3427.500000     4.000000\n",
       "max     425.000000  26668.000000     5.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resum.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-cooperation",
   "metadata": {},
   "source": [
    "### Neteja de text.\n",
    "Prepararem aquest conjunt de textos per entrenar un model per predir la puntuació de cada crítica a partir del text de la crítica.\\\n",
    "Realitzarem el següent processat:\n",
    "- Introduïm un espai després de determinats signes de puntuació (\".\", \"?\") perquè el tokenitzat siga correcte.\n",
    "- Separar el text a *tokens*.\n",
    "- Eliminar els *tokens* de tipus *stop-word*, signes de puntuació, espais o amb longitud de 1.\n",
    "- Convertir les entitats de tipus `PER` al token *persona*.\n",
    "- Lematitzar el text i convertir minúscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "olympic-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "\n",
    "def normalitzar(text):\n",
    "    # Separem després de certs signes de puntuació.\n",
    "    text = re.sub(r\"([\\.\\?])\", r\"\\1 \", text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [t.lemma_.lower() if not t.ent_type_=='PER' else '_PERSONA_'\n",
    "              for t in doc if not t.is_punct and not t.is_stop and not t.is_space and len(t.text)>1]\n",
    "    output = ' '.join(tokens)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-contract",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament a la primera crítica del conjunt de train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "retained-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text original de la primera crítica:\n",
      "<review author=\"Torbe\" title=\"La guerra de los mundos\" rank=\"1\" maxRank=\"5\" source=\"muchocine\">\n",
      "\t<summary>Hasta los cojones de los yankis</summary>\n",
      "\t<body>Cada vez me gusta menos el cine de masas. Las peliculas que ven todo el mundo me parecen cada vez mas coñazo y mas insufribles. No se porqué pero siempre el prota es tonto del culo y tiene suerte, y al final de la peli, cuando ha logrado vencer al mal, se convierte en listo, y las chorradas que hacia al comienzo de la pelicula se esfuman como por arte de magia. Se vuelve maduro e inteligente.Esta peli de Spielberg es mas de lo mismo, huir y huir y que no le den ni un solo tiro. Además el cabron ha metido a un par de actores que es como para echarles de comer aparte. La niña, una vieja metida en el cuerpo de una niña, porque solo hay que verle hablar (en version original claro) para darse cuenta que estamos ante uno de los grandes freaks del cine. Se creeran que hace gracia la nena cuando habla igual que su puta madre, pero a mi me causa pavor. Ver a una cria que habla como una persona madura es algo horroroso. Los niños son niños y verlos fuera de su rol asusta.Luego esta el hijo adolescente que tiene el Cruise. Otro subnormal que es para darle de bofetadas hasta que se te vea el hueso a la mano. Fiel reflejo de lo que se denomina manipulacion militar, el chico quiere matar a los bichos sin ningun arma, hale, a lo loco y sin pensar, venga, a saco.Que quereis que os diga, pero a mi eso me parece fanatismo y locura. Que alguien quiera ir a luchar sin medios es ir a una muerte segura, por eso me jode sobremanera que al final de la pelicula aparezca el mongo este y sus abuelos y toda la familia. Se salva todo dios, y encima la vieja aparece en traje de los domingos y toda maquillada.¿Pero que sinsorgada es esta? Solo falta que Cruise vuelva con su exmujer y que el mundo sea mucho mejor. Tranquilos, os cuento el final de la peli pero no pasa nada, todas las pelis acaban de igual manera, y decir eso no resta misterio al bodrio este. Yo no se de que va Spielberg, pero a este paso se va a convertir tan solo en un director que es bueno haciendo efectos especiales, porque lo que es contando historias?¿No os disteis cuenta de que los norteamericanos no saben guardar la compostura? Que todo lo dicen gritando. En momentos dificiles lo mas sensato es tranquilizarse y no discutir por chorradas, y en toda la puta peli no paran de gritar y pegarse entre ellos.Me hace gracia como al comienzo de la peli, cuando el suelo empieza a resquebrajarse, la gente se queda ahí a mirar que pasa. Joder, no se ellos, pero yo ya estaria corriendo como una puta desde hace tiempo, e escondiéndome. Si veo que un bicho esta lanzando rayos y machacando a to dios, me escondo.En fin, que para que seguir hablando de esta mierda, si lo que querian ya lo han conseguido, que es que pagasemos la entrada, por eso creo que va a ir al cine su prima, a partir de ahora me lo bajare todo de internet, que por lo que se ve, la calidad de las pelis es aceptable, y puedo pasarla palante. Hoy en dia no tener un mando con el forward en la mano es morir.</body>\n",
      "</review>\n",
      "\n",
      "\n",
      "Text normalitzat de la primera crítica:\n",
      "review author=\"torbe title=\"la guerra de el mundo rank=\"1 maxrank=\"5 source=\"muchocine summary hasta el cojón de el yankis</summary body gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli _PERSONA_ huir huir dar tiro cabron meter par actor echarl comer aparte el niña viejo metida cuerpo niña ver él hablar version original dar él freaks cine creeran gracia nena puta madre causar pavor cria persona maduro horroroso el niño niño ver él rol asusta luego este el hijo adolescente cruise subnormal dar él bofetada ver hueso mano _PERSONA_ reflejo denominar manipulacion militar chico matar bicho ningun arma hale loco pensar venir saco quereis decir fanatismo locura alguien queír luchar medio muerte seguro jodar sobremanera pelicula aparezco mongo abuelo familia salir dio viejo aparecer traje domingo maquillado ¿pero que sinsorgado faltar _PERSONA_ volver exmujer mundo tranquilo contar peli pasar peli acabar restar misterio bodrio _PERSONA_ paso convertir director efecto especial contar historia no tú disteis norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él yo hacer gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar joder estario correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios escondo seguir hablar mierda _PERSONA_ conseguir pagasemos entrada cine prima bajarar internet ver calidad peli aceptable pasar él palante mando forward mano morir /body /review\n"
     ]
    }
   ],
   "source": [
    "ruta_fitxer = \"critiques/critiques/train/2.xml\"\n",
    "\n",
    "# Llegim el contingut de la crítica\n",
    "with open(ruta_fitxer, 'r', encoding='latin1') as file:\n",
    "    primera_critica = file.read()\n",
    "\n",
    "# Aplicar normalització\n",
    "critica_neta = normalitzar(primera_critica)\n",
    "\n",
    "print(\"Text original de la primera crítica:\")\n",
    "print(primera_critica)\n",
    "print(\"\\nText normalitzat de la primera crítica:\")\n",
    "print(critica_neta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-cooling",
   "metadata": {},
   "source": [
    "### Extracció de característiques *sparse*.\n",
    "Calcularem les matrius de característiques *bag-of-words* i *tfidf* del conjunt de textos anterior.\\\n",
    "Farem servir la llibreria `scikit-learn` per vectoritzar els documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "funny-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per no haver de carregar totes les crítiques en memòria, creem un generador que torna iterativament el\n",
    "# text processat de cada crítica.\n",
    "\n",
    "def critiques_folder(folder):\n",
    "    for c in parse_folder(folder):\n",
    "        yield normalitzar(c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-reflection",
   "metadata": {},
   "source": [
    "Comprova'n el funcionament generant el text normalitzat de la primera crítica del conjunt de *train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "indirect-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text normalitzat de la primera crítica del conjunt de train:\n",
      "gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli _PERSONA_ huir huir dar tiro cabron meter par actor echarl comer aparte el niña viejo metida cuerpo niña ver él hablar version original dar él freaks cine creeran gracia nena puta madre causar pavor cria persona maduro horroroso el niño niño ver él rol asusta luego este el hijo adolescente cruise subnormal dar él bofetada ver hueso mano _PERSONA_ reflejo denominar manipulacion militar chico matar bicho ningun arma hale loco pensar venir saco quereis decir fanatismo locura alguien queír luchar medio muerte seguro jodar sobremanera pelicula aparezco mongo abuelo familia salir dio viejo aparecer traje domingo maquillado ¿pero que sinsorgado faltar _PERSONA_ volver exmujer mundo tranquilo contar peli pasar peli acabar restar misterio bodrio _PERSONA_ paso convertir director efecto especial contar historia no tú disteis norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él yo hacer gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar joder estario correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios escondo seguir hablar mierda _PERSONA_ conseguir pagasemos entrada cine prima bajarar internet ver calidad peli aceptable pasar él palante mando forward mano morir\n"
     ]
    }
   ],
   "source": [
    "folder_train = \"critiques/critiques/train\"\n",
    "# \n",
    "# Inicialitzem el generador per obtenir el text normalitzat de les crítiques del conjunt de train\n",
    "generador_critiques = critiques_folder(folder_train)\n",
    "\n",
    "# Obtenim el text normalitzat de la primera crítica\n",
    "text_normalitzat_primera_critica = next(generador_critiques)\n",
    "\n",
    "print(\"Text normalitzat de la primera crítica del conjunt de train:\")\n",
    "print(text_normalitzat_primera_critica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-aggregate",
   "metadata": {},
   "source": [
    "Vectoritzem tot el conjunt de dades usant les funcions de `scikit-learn`.\\\n",
    "Aquestes funcions admeten un objecte `generator` com a argument dʻentrada.\\\n",
    "El vectoritzador ha d'aprendre només sobre el conjunt de TRAIN i després aplicar-se al conjunt de TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "graduate-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2714, 2746)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=0.01)\n",
    "\n",
    "X_train_vectorized = vect.fit_transform(critiques_folder(\"critiques/critiques/train\"))\n",
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-massage",
   "metadata": {},
   "source": [
    "Aplica el vectoritzador al conjunt de TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "distinguished-michael",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164, 2746)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized = vect.transform(critiques_folder(\"critiques/critiques/test\"))\n",
    "X_test_vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-office",
   "metadata": {},
   "source": [
    "### Variable *target*.\n",
    "Creem la llista d'etiquetes de cada crítica a partir dels fitxers. Definim 3 classes per a les 5 puntuacions per facilitar-ne la classificació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "chronic-smooth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2714"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuacions = {1: 'NEG', 2:'NEG', 3:'NEU', 4:'POS', 5:'POS'}\n",
    "y_train = []\n",
    "for c in parse_folder(\"critiques/critiques/train\"):\n",
    "    y_train.append(puntuacions[c[3]])\n",
    "    \n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-disney",
   "metadata": {},
   "source": [
    "#### Exercici.\n",
    "Crea la llista d'etiquetes de test `y_test` a partir dels fitxers al directori `\"critiques/test\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "under-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1164"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuacions = {1: 'NEG', 2:'NEG', 3:'NEU', 4:'POS', 5:'POS'}\n",
    "y_test = []\n",
    "for c in parse_folder(\"critiques/critiques/test\"):\n",
    "    y_test.append(puntuacions[c[3]])\n",
    "    \n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-stake",
   "metadata": {},
   "source": [
    "### Entrenament del classificador.\n",
    "Entrenarem amb diferents models de classificador sobre el conjunt de crítiques i compara el seu rendiment.\\\n",
    "#### Exercici\n",
    "Prova a classificar les crítiques amb els models de regressió logística, Naïve Bayes i SVM lineal.\\\n",
    "Utilitza la mètrica `classification_report` de `scikit-learn` per comparar els models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "together-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe de classificació per a la regressió logística amb Bag-of-Words:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.65      0.62      0.63       395\n",
      "         NEU       0.45      0.47      0.46       374\n",
      "         POS       0.59      0.60      0.60       395\n",
      "\n",
      "    accuracy                           0.56      1164\n",
      "   macro avg       0.57      0.56      0.56      1164\n",
      "weighted avg       0.57      0.56      0.57      1164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajlmt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Modelo BoW-LR\n",
    "# COMPLETAR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Creem un classificador de regressió logística\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# Entrenem el classificador amb les dades vectoritzades del conjunt de train\n",
    "clf_lr.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Fem prediccions sobre les dades vectoritzades del conjunt de test\n",
    "prediccions_lr = clf_lr.predict(X_test_vectorized)\n",
    "\n",
    "# Avaluem el rendiment del classificador utilitzant classification_report\n",
    "informe_lr = classification_report(y_test, prediccions_lr)\n",
    "\n",
    "# Mostrem l'informe de classificació\n",
    "print(\"Informe de classificació per a la regressió logística amb Bag-of-Words:\\n\", informe_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "interim-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe de classificació per a Naïve Bayes amb Bag-of-Words:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.68      0.68      0.68       395\n",
      "         NEU       0.51      0.45      0.48       374\n",
      "         POS       0.59      0.66      0.62       395\n",
      "\n",
      "    accuracy                           0.60      1164\n",
      "   macro avg       0.59      0.60      0.59      1164\n",
      "weighted avg       0.60      0.60      0.60      1164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Modelo BoW-NB\n",
    "# COMPLETAR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Creem un classificador de Naïve Bayes\n",
    "clf_nb = MultinomialNB()\n",
    "\n",
    "# Entrenem el classificador amb les dades vectoritzades del conjunt de train\n",
    "clf_nb.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Fem prediccions sobre les dades vectoritzades del conjunt de test\n",
    "prediccions_nb = clf_nb.predict(X_test_vectorized)\n",
    "\n",
    "# Avaluem el rendiment del classificador utilitzant classification_report\n",
    "informe_nb = classification_report(y_test, prediccions_nb)\n",
    "\n",
    "# Mostrem l'informe de classificació\n",
    "print(\"Informe de classificació per a Naïve Bayes amb Bag-of-Words:\\n\", informe_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "genuine-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajlmt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe de classificació per a SVM lineal amb Bag-of-Words:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.62      0.58      0.60       395\n",
      "         NEU       0.45      0.45      0.45       374\n",
      "         POS       0.56      0.59      0.58       395\n",
      "\n",
      "    accuracy                           0.54      1164\n",
      "   macro avg       0.54      0.54      0.54      1164\n",
      "weighted avg       0.54      0.54      0.54      1164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajlmt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Modelo BoW-SVM\n",
    "# COMPLETAR\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Creem un classificador SVM lineal\n",
    "clf_svm = LinearSVC()\n",
    "\n",
    "# Entrenem el classificador amb les dades vectoritzades del conjunt de train\n",
    "clf_svm.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Fem prediccions sobre les dades vectoritzades del conjunt de test\n",
    "prediccions_svm = clf_svm.predict(X_test_vectorized)\n",
    "\n",
    "# Avaluem el rendiment del classificador utilitzant classification_report\n",
    "informe_svm = classification_report(y_test, prediccions_svm)\n",
    "\n",
    "# Mostrem l'informe de classificació\n",
    "print(\"Informe de classificació per a SVM lineal amb Bag-of-Words:\\n\", informe_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-conversation",
   "metadata": {},
   "source": [
    "## Part 2: conjunt \"20Newsgroups\".\n",
    "En aquesta part crearem un classificador de textos multiclasse en anglès usant `scikit-learn` usant models BoW, TF-IDF i *averaged word vectors*.\n",
    "\n",
    "Farem servir com a conjunt de prova el dataset *20newsgroups*, que consisteix en unes 18000 notícies d'un fòrum de discussió en anglès dividides en 20 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-warrior",
   "metadata": {},
   "source": [
    "### Descàrrega del dataset.\n",
    "Ens descarreguem el dataset amb la llibreria `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "specified-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregats 18331 documents\n",
      "Classes:\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Document exemple:\n",
      " the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "\n",
      "Classe: 19 (talk.religion.misc)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data\n",
    "    \n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n",
    "    \n",
    "    \n",
    "dataset = get_data()\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "print(f'\\nCarregats {len(corpus)} documents')\n",
    "print('Classes:\\n',dataset.target_names)\n",
    "print('\\nDocument exemple:\\n', corpus[10])\n",
    "print(f'\\nClasse: {labels[10]} ({dataset.target_names[labels[10]]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-generic",
   "metadata": {},
   "source": [
    "La quantitat de documents dins de cada classe està prou balancejada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "simple-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       clase    N\n",
       "0           rec.sport.hockey  975\n",
       "1   comp.sys.ibm.pc.hardware  964\n",
       "2      talk.politics.mideast  919\n",
       "3      comp.sys.mac.hardware  929\n",
       "4            sci.electronics  958\n",
       "5         talk.religion.misc  606\n",
       "6                  sci.crypt  962\n",
       "7                    sci.med  960\n",
       "8                alt.atheism  779\n",
       "9            rec.motorcycles  969\n",
       "10                 rec.autos  937\n",
       "11            comp.windows.x  982\n",
       "12             comp.graphics  955\n",
       "13                 sci.space  955\n",
       "14        talk.politics.guns  886\n",
       "15              misc.forsale  959\n",
       "16        rec.sport.baseball  958\n",
       "17        talk.politics.misc  756\n",
       "18   comp.os.ms-windows.misc  947\n",
       "19    soc.religion.christian  975"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame([(dataset.target_names[k], v) for k,v in Counter(labels).items()], columns=['clase', 'N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-delaware",
   "metadata": {},
   "source": [
    "### Submostreig del conjunt de dades.\n",
    "Per no treballar amb un conjunt tan gran ens quedarem amb només 2000 mostres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "military-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "corpus_sm, labels_sm = resample(corpus, labels, n_samples=2000, replace=False,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-paraguay",
   "metadata": {},
   "source": [
    "#### Exercici.\n",
    "Divideix el conjunt d'entrenament submostrejat en entrenament i test (30%) per entrenar i validar els models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "handy-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamany de TRAIN: 1400\n",
      "Tamany de TEST: 600\n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus_sm, labels_sm, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f'\\nTamany de TRAIN: {len(train_corpus)}')\n",
    "print(f'Tamany de TEST: {len(test_corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-deployment",
   "metadata": {},
   "source": [
    "Mostra quantes mostres de cada classe tenim a TRAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "stainless-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitat de mostres de cada classe a TRAIN:\n",
      "                      Classe  Quantitat\n",
      "0            sci.electronics         76\n",
      "1               misc.forsale         66\n",
      "2      comp.sys.mac.hardware         68\n",
      "3         talk.politics.misc         51\n",
      "4                alt.atheism         50\n",
      "5    comp.os.ms-windows.misc         88\n",
      "6   comp.sys.ibm.pc.hardware         74\n",
      "7             comp.windows.x         77\n",
      "8                  sci.space         89\n",
      "9                  rec.autos         77\n",
      "10        rec.sport.baseball         68\n",
      "11    soc.religion.christian         65\n",
      "12     talk.politics.mideast         76\n",
      "13                   sci.med         68\n",
      "14           rec.motorcycles         77\n",
      "15        talk.politics.guns         64\n",
      "16          rec.sport.hockey         77\n",
      "17        talk.religion.misc         48\n",
      "18                 sci.crypt         71\n",
      "19             comp.graphics         70\n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "train_df = pd.DataFrame([(dataset.target_names[label], count) for label, count in Counter(train_labels).items()], columns=['Classe', 'Quantitat'])\n",
    "\n",
    "print(\"Quantitat de mostres de cada classe a TRAIN:\")\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-newcastle",
   "metadata": {},
   "source": [
    "### Pre-processament del text.\n",
    "Realitzem una neteja del text (treiem signes de puntuació i espais) i ens quedem amb el lema de cada paraula en minúscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "steady-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "nlp=spacy.load('en_core_web_md')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    '''Netegem i normalitzem un document passat com a string'''\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuación/espacios\n",
    "    filtered_tokens = [t for t in tokens if not t.is_punct and not t.is_space and not t.is_digit]\n",
    "    #cogemos el lemma\n",
    "    lemmas = []\n",
    "    for tok in filtered_tokens:\n",
    "        lemma = re.sub('[{}]'.format(re.escape(string.punctuation)), '', tok.lemma_.lower()) if tok.lemma_ != \"-PRON-\" else tok.lower_\n",
    "        if len(lemma)>2:\n",
    "            lemmas.append(lemma)\n",
    "    # juntamos de nuevo en una cadena\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    '''Apliquem la funció de normalització sobre el corpus passat com a llista de string'''\n",
    "    return [normalize_document(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-cigarette",
   "metadata": {},
   "source": [
    "Per exemple veiem el document núm. 15 normalitzat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "signed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Because I'm a guy and most of my pillions are female. \n",
      "\n",
      "Also, the other reasons, like having an idea where you passengers\n",
      "weight is, it being a more comfortable position for the passenger,\n",
      "and it being a more stable configuration all come into it as well.\n",
      "\n",
      "Holding the grab rail is a great idea only for braking, when you\n",
      "don't want the pillion to slide forward into you, otherwise I don't\n",
      "find it works well.\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "print(corpus_sm[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "automatic-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because guy and most pillion female also the other reason like have idea where you passenger weight more comfortable position for the passenger and more stable configuration all come into well hold the grab rail great idea only for braking when you not want the pillion slide forward into you otherwise not find work well\n"
     ]
    }
   ],
   "source": [
    "# normalitzat\n",
    "print(normalize_document(corpus_sm[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-nebraska",
   "metadata": {},
   "source": [
    "Normalitzem tot el conjunt de textos. Ho guardem com una llista en lloc de `generator` perquè l'hem d'utilitzar múltiples vegades i no volem haver de normalitzar tot el corpus cada cop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cardiac-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_corpus = list(map(normalize_document, train_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-metadata",
   "metadata": {},
   "source": [
    "#### Exercici.\n",
    "Crea una llista per al conjunt de TEST normalitzat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "quarterly-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "# Normalitzem tot el conjunt de textos de test\n",
    "norm_test_corpus = list(map(normalize_document, test_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-prior",
   "metadata": {},
   "source": [
    "## Models BoW i TF-IDF.\n",
    "Instanciem els vectoritzadors per obtenir les característiques BoW i TF-IDF.  \n",
    "Fem servir el paràmetre max_df=0.9 per eliminar els stop-words com les paraules que apareixen almenys al 90% dels documents i el paràmetre min_df=0.01 per eliminar les paraules que no apareixen almenys en un 1% dels documents.\\\n",
    "Fem servir el model `TfidfTransformer` per calcular la matriu TF-IDF a partir del BoW i no haver de repetir tot l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "accepted-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9)\n",
    "\n",
    "tfidf_vectorizer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-costs",
   "metadata": {},
   "source": [
    "## Model Averaged Word Vectors.  \n",
    "Per calcular els models basats en WV fem servir el model gloVe pre-entrenat a `spaCy`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-friend",
   "metadata": {},
   "source": [
    "Calculem dos models basats en word-vectors:  \n",
    "* el vector mitjana dels WV de tots els tokens amb el mateix pes per a totes les paraules.  \n",
    "* ponderant el WV de cada paraula pel terme de freqüència inversa de document (IDF).  \n",
    "\n",
    "Definim les funcions per calcular aquestes dues matrius de característiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "southeast-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word_vectorizer(corpus):\n",
    "    '''Aplica la funció de càlcul del WV mitjana a tots els\n",
    "     documents del corpus (corpus és una llista de docs com a strings)'''\n",
    "    features = [nlp(doc).vector\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(doc, word_tfidf_map):\n",
    "    '''Aplica la funció de càlcul del WV ponderat per TF-IDF\n",
    "     de cada token a un document'''\n",
    "    tokens = doc.split()\n",
    "\n",
    "    feature_vector = np.zeros((nlp.vocab.vectors_length,),dtype=\"float64\")\n",
    "    wts = 0.      \n",
    "    for word in tokens:\n",
    "        if nlp.vocab[word].has_vector and word_tfidf_map.get(word, 0): #sólo considera palabras conocidas\n",
    "            weighted_word_vector = word_tfidf_map[word] * nlp.vocab[word].vector\n",
    "            wts = wts + 1\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, word_tfidf_map):\n",
    "    '''Aplica la funció de càlcul del WV ponderat per TF-IDF a tots els\n",
    "     documents del corpus'''                                       \n",
    "    features = [tfidf_wtd_avg_word_vectors(doc, word_tfidf_map)\n",
    "                    for doc in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-fellowship",
   "metadata": {},
   "source": [
    "### Extracció de característiques.\n",
    "Extraiem característiques amb els diferents models al nostre conjunt d´entrenament.\n",
    "#### Exercici.\n",
    "Extreu les característiques BoW, TF-IDF, *averaged Word Vectors* i *TFIDF-weighted WV* dels conjunts normalitzats de train i test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "integrated-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Definir el vectorizador BoW\n",
    "bow_vectorizer = CountVectorizer(min_df=0.01, max_df=0.9)\n",
    "\n",
    "# Características Bag of Words\n",
    "bow_train_features = bow_vectorizer.fit_transform(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus) \n",
    "\n",
    "# Definir y aplicar el transformador TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_train_features = tfidf_transformer.fit_transform(bow_train_features)\n",
    "tfidf_test_features = tfidf_transformer.transform(bow_test_features)\n",
    "\n",
    "# Características Averaged Word Vector\n",
    "avg_wv_train_features = averaged_word_vectorizer(norm_train_corpus)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(norm_test_corpus)\n",
    "\n",
    "# Características TF-IDF Weighted Average Word Vector\n",
    "word_tfidf_map = {key:value for (key, value) in zip(bow_vectorizer.get_feature_names_out(), tfidf_transformer.idf_)}\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(norm_train_corpus, word_tfidf_map)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(norm_test_corpus, word_tfidf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "compact-devices",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1237)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "liable-looking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1237)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "painted-nickname",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "seeing-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 300)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_wv_test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-walter",
   "metadata": {},
   "source": [
    "### Classificació.\n",
    "Apliquem diferents classificadors a cada model per veure quin funciona millor amb les nostres dades.  \n",
    "Primer definim unes funcions per entrenar i mesurar el rendiment dels classificadors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "driven-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculem diferents mètriques sobre el\n",
    "     rendiment del model. Torna un diccionari\n",
    "     amb els paràmetres mesurats\"\"\"\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        3),\n",
    "        'Precision': np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'Recall': np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3),\n",
    "    'F1 Score': np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted',\n",
    "                                               zero_division=0),\n",
    "                        3)}\n",
    "                        \n",
    "\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    \"\"\"Funció que entrena un model de classificació sobre\n",
    "     un conjunt d'entrenament, ho aplica sobre un conjunt\n",
    "     de test i torna la predicció sobre el conjunt de test\n",
    "     i les mètriques de rendiment\"\"\"\n",
    "    # genera modelo    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predice usando el modelo sobre test\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evalúa rendimiento de la predicción   \n",
    "    metricas = get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions, metricas    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-logging",
   "metadata": {},
   "source": [
    "Anem a entrenar sobre el conjunt de train i avaluem al conjunt de test. Desem mètriques en una llista i resultats en una altra per mostrar resum.\n",
    "#### Exercici.\n",
    "Completa el codi per entrenar els models basats en TF-IDF, averaged WV i TFIDF-weigthed WV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "color-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear')\n",
    "modelNB = GaussianNB()\n",
    "modelSVM = SGDClassifier(loss='hinge', max_iter=1000)\n",
    "modelRBFSVM = SVC(gamma='scale', C=2)\n",
    "\n",
    "modelos = [('Logistic Regression', modelLR),\n",
    "           ('Naive Bayes', modelNB),\n",
    "           ('Linear SVM', modelSVM),\n",
    "           ('Gauss kernel SVM', modelRBFSVM)]\n",
    "\n",
    "metricas = []\n",
    "resultados = []\n",
    "\n",
    "# Models amb característiques BoW.\n",
    "bow_train_features = bow_train_features.toarray()\n",
    "bow_test_features = bow_test_features.toarray()\n",
    "\n",
    "# Models amb característiques averaged word vectors.\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo'] = f'{m} Averaged Word Vectors'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n",
    "\n",
    "# Models amb característiques tf idf weighted average word vectors.\n",
    "for m, clf in modelos:\n",
    "    prediccion, metrica = train_predict_evaluate_model(classifier=clf,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "    metrica['modelo'] = f'{m} TF-IDF Weighted WV'\n",
    "    resultados.append(prediccion)\n",
    "    metricas.append(metrica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-finnish",
   "metadata": {},
   "source": [
    "Converteix la llista de mètriques en un DataFrame per observar-ne els valors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "latter-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Precision  Recall  F1 Score  \\\n",
      "0     0.518      0.522   0.518     0.514   \n",
      "1     0.298      0.324   0.298     0.292   \n",
      "2     0.468      0.539   0.468     0.452   \n",
      "3     0.463      0.470   0.463     0.446   \n",
      "4     0.383      0.392   0.383     0.382   \n",
      "5     0.265      0.293   0.265     0.236   \n",
      "6     0.360      0.490   0.360     0.338   \n",
      "7     0.398      0.405   0.398     0.376   \n",
      "\n",
      "                                      modelo  \n",
      "0  Logistic Regression Averaged Word Vectors  \n",
      "1          Naive Bayes Averaged Word Vectors  \n",
      "2           Linear SVM Averaged Word Vectors  \n",
      "3     Gauss kernel SVM Averaged Word Vectors  \n",
      "4     Logistic Regression TF-IDF Weighted WV  \n",
      "5             Naive Bayes TF-IDF Weighted WV  \n",
      "6              Linear SVM TF-IDF Weighted WV  \n",
      "7        Gauss kernel SVM TF-IDF Weighted WV  \n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "# Convertir la lista de métricas en un DataFrame\n",
    "metricas_df = pd.DataFrame(metricas)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(metricas_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-patrol",
   "metadata": {},
   "source": [
    "Ordena les mètriques per `accuracy` i mostra el millor resultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "offshore-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor resultado:\n",
      "Accuracy                                         0.518\n",
      "Precision                                        0.522\n",
      "Recall                                           0.518\n",
      "F1 Score                                         0.514\n",
      "modelo       Logistic Regression Averaged Word Vectors\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# COMPLETAR\n",
    "# Ordenar las métricas por accuracy en orden descendente\n",
    "metricas_df_sorted = metricas_df.sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "# Mostrar el mejor resultado (la primera fila después de ordenar)\n",
    "mejor_resultado = metricas_df_sorted.iloc[0]\n",
    "\n",
    "# Imprimir el mejor resultado\n",
    "print(\"Mejor resultado:\")\n",
    "print(mejor_resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-sudan",
   "metadata": {},
   "source": [
    "Visualitza la matriu de confusió de la predicció per al millor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "other-addiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0   1   2   3   4   5   6   7   8   9   10  11  12  \\\n",
       "alt.atheism                9   1   0   0   0   0   1   1   0   0   0   0   0   \n",
       "comp.graphics              0  16   5   1   2   5   0   0   0   0   0   0   1   \n",
       "comp.os.ms-windows.misc    0   4  14   2   1   4   1   0   0   0   0   0   1   \n",
       "comp.sys.ibm.pc.hardware   0   2   4  21   4   1   0   0   0   0   0   1   0   \n",
       "comp.sys.mac.hardware      0   2   2   7  10   1   1   1   0   0   0   2   5   \n",
       "comp.windows.x             0   4   4   1   1  15   0   0   0   0   0   1   1   \n",
       "misc.forsale               0   0   0   2   1   0  16   3   2   0   0   0   3   \n",
       "rec.autos                  0   0   0   1   0   0   0  12   4   0   0   0   2   \n",
       "rec.motorcycles            1   1   0   1   0   0   0   1  13   1   0   0   1   \n",
       "rec.sport.baseball         0   0   1   0   1   1   0   0   0  18   5   0   1   \n",
       "rec.sport.hockey           0   1   1   0   0   0   0   1   1   4  18   1   2   \n",
       "sci.crypt                  0   3   0   2   1   2   0   0   1   0   1  12   4   \n",
       "sci.electronics            0   0   2   4   0   0   1   1   0   0   0   2  19   \n",
       "sci.med                    0   1   0   0   0   0   0   0   0   1   0   0   1   \n",
       "sci.space                  0   0   0   1   1   0   0   1   2   0   0   1   1   \n",
       "soc.religion.christian     4   0   0   0   0   0   0   0   0   1   0   0   0   \n",
       "talk.politics.guns         2   0   0   0   0   0   0   0   2   0   0   3   1   \n",
       "talk.politics.mideast      1   0   0   0   0   1   0   0   0   1   0   0   1   \n",
       "talk.politics.misc         0   0   0   0   0   0   0   3   0   1   2   1   0   \n",
       "talk.religion.misc         2   0   0   0   1   0   0   1   1   0   0   0   0   \n",
       "\n",
       "                          13  14  15  16  17  18  19  \n",
       "alt.atheism                0   0   5   1   2   3   4  \n",
       "comp.graphics              0   0   0   1   0   1   0  \n",
       "comp.os.ms-windows.misc    0   0   0   0   0   0   0  \n",
       "comp.sys.ibm.pc.hardware   0   0   0   0   1   0   0  \n",
       "comp.sys.mac.hardware      0   0   0   0   1   0   0  \n",
       "comp.windows.x             0   1   0   0   0   0   0  \n",
       "misc.forsale               1   0   0   0   2   0   0  \n",
       "rec.autos                  0   1   0   1   0   0   0  \n",
       "rec.motorcycles            0   1   0   0   0   1   0  \n",
       "rec.sport.baseball         3   0   0   0   1   1   1  \n",
       "rec.sport.hockey           2   0   1   1   0   0   0  \n",
       "sci.crypt                  1   0   1   2   1   0   1  \n",
       "sci.electronics            0   3   1   0   1   0   1  \n",
       "sci.med                   28   0   0   0   1   0   0  \n",
       "sci.space                  0  23   1   1   0   0   0  \n",
       "soc.religion.christian     0   1  15   0   1   2   5  \n",
       "talk.politics.guns         1   2   0  17   4   6   1  \n",
       "talk.politics.mideast      1   1   3   2  26   1   0  \n",
       "talk.politics.misc         1   2   1   6   2   6   1  \n",
       "talk.religion.misc         1   0   7   0   2   1   3  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriu de confusió\n",
    "import pandas as pd\n",
    "\n",
    "indice_mejor_modelo = metricas_df_sorted.index[0]\n",
    "cm = metrics.confusion_matrix(test_labels, resultados[indice_mejor_modelo])\n",
    "pd.DataFrame(cm, index=dataset.target_names, columns=modelSVM.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-consultancy",
   "metadata": {},
   "source": [
    "### Exercici.\n",
    "Divideix el conjunt total de mostres (`corpus` i `labels`) en entrenament i test (30%) i entrena el millor model obtingut, per veure si es milloren els resultats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
